# Gaussian Process

## Table of Contents

 - [Terminology](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#terminology)
  - [Introduction](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#introduction)
  - [Describing a Gaussian Process](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#describing-a-gaussian-process)
  - [Acknowledgements](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#acknowledgements)

## Terminology

Before we begin, it's important to understand the following terms:
 
 - [Gaussian distribution](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#gaussian-distribution)
 - [Multivariate normal distribution](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#multivariate-normal-distribution)

### Gaussian Distribution

The gaussian distribution, also called the normal distribution, is a continuous probability distribution that is common in both theory and practice.

A random variable ð‘‹ with a normal distribution is described with two parameters:
 - the mean ðœ‡
 - the variance ðœŽÂ²
 
We write ð‘‹ ~ ð’©(ðœ‡, ðœŽÂ²).
 
The distribution is always symmetric about the mean. Approximately, 95% of the area of the distribution is within two standard deviations of the mean.

As the standard deviation ðœŽ increases, the peak at ðœ‡ becomes lower and more area is distributed to the tails.

![Gaussian distributions.](https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/700px-Normal_Distribution_PDF.svg.png)

### Multivariate normal distribution

This is a generalisation on the (univariate) normal distribution, applied to higher dimensions.

![Sample points in a multivariate normal distribution.](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/MultivariateNormal.png/600px-MultivariateNormal.png)

The random variable ð‘ with a standard normal distribution is described by ð‘ ~ ð’©(0, 1).

## Introduction

A gaussian process (GP) is often referred to as the infinite-dimensional extension of the multi-variate normal distribution. However, in practice we typically don't work with random variables with infinitely many components. When we work with GPs, the thinking is that we observe some finite-dimensional subset of infinite-dimensional data, and that this finite subset follows a multivariate normal distribution. We call this finite subset the **finite dimensional distribution**.

Okay... but what does this actually mean?

Well, imagine if we measure the temperature in a certain shop every day at 10am for 6-weeks. This would yield a (6 Ã— 7 =) **42** dimensional vector:

 - { ð‘¥â‚, ð‘¥â‚‚, ..., ð‘¥â‚„â‚‚ }, _this is the finite dimensional distribution_.

In reality, however, the temperature is continuous, and and our choice to only take a measurement at 10am is completely arbitrary. How would the data change if we took measurements at 10pm instead? If we model this data with a GP, we are making the assumption that each of these possible data collection schemes would yield data from a multivariate normal distribution.

As a result of this, it makes sense to think of GPs as functions.

Formally, we'd say that a function ð‘“ is a GP if any set of finite values ð‘“(ð‘¥â‚), ð‘“(ð‘¥â‚‚), ..., ð‘“(ð‘¥â‚™) has a [multivariate normal distribution](https://github.com/benjaminhadfield/machine-learning/tree/master/src/gaussian_process#multivariate-normal-distribution) where our inputs {ð‘¥â‚, ð‘¥â‚‚, ..., ð‘¥â‚™} correspond to objects (usually vectors) from some arbitrarily sized domain (in our case, that domain is the temperature of the shop at 10am over the 6-week period, and is the finite subset of the infinite domain of all temperatures in the shop).

So in our temperature example, { ð‘¥â‚, ð‘¥â‚‚, ..., ð‘¥â‚„â‚‚ } corresponds to days in a 6-week period, and ð‘“(ð‘¥â‚™) indicates the temperature at 10am on day ð‘›.

## Describing a Gaussian Process

### Parameters

A GP is specified by two functions:
 
 - a mean function, ð‘š(ð‘¥)
 - a covariance function, also called a kernel, ð‘˜(ð‘¥, ð‘¥Ê¹)
 
### Effect on the output
 
The shape and smoothness of the function is determined by the covariance function, as it controls the correlation between all pairs of output values. That is, for any given ð‘¥ and ð‘¥Ê¹, ð‘š(ð‘¥) = ð”¼[ð‘“(ð‘¥)], where ð”¼[ð‘“(ð‘¥)] denotes the expectation of ð‘“(ð‘¥)., and ð‘˜(ð‘¥, ð‘¥Ê¹) = ð¶ð‘œð‘£(ð‘“(ð‘¥), ð‘“(ð‘¥)Ê¹).

Thus if ð‘˜(ð‘¥, ð‘¥Ê¹) is large when ð‘¥ and ð‘¥Ê¹ are near each other, the function will be more smooth, whilst smaller kernel values imply a more jagged function.

So, given a mean function ð‘“(ð‘¥) and a kernel ð‘˜(ð‘¥, ð‘¥Ê¹), we can sample from any GP.

Lets say we want to evaluate the function at ð‘µ inputs, each of which has dimension ð‘«. Firstly, we create a matrix ð‘¿ âˆŠ â„á´ºá´°, where each row corresponds to an input we would like to sample from. We can then evaluate the mean function ð‘“(ð‘¥) at all inputs, denoted by ð’Žð±, which is a vector of length ð‘µ, and the kernel matrix corresponding to ð‘¿, denoted by ð‘²ð±ð±.

More generally, for any two sets of input data, ð‘¿ and ð‘¿Ê¹, we define ð‘²ð±ð±â€² to be the matrix where the (ð‘–, ð‘—) element is ð’Œ(ð‘¥áµ¢, ð‘¥â±¼). Finally, we can sample a random vector ð‘“ from a multivariate normal distribution: ð‘“ ~ ð’©(ð’Žð±, ð‘²ð±ð±). By construction, ð”¼(ð‘“(ð‘¥â‚™)) = ð’Ž(ð‘¥â‚™) for all ð‘› and ð¶ð‘œð‘£(ð‘“(ð‘¥â‚™), ð‘“(ð‘¥â‚˜)Ê¹) = ð’Œ(ð’™â‚™, ð’™â‚˜) for all pairs ð‘›, ð‘š. Because this vector has a multivariate normal distribution, all subsets also follow a multivariate distribution, thereby fulfilling the definition of a GP.

### Kernels

A kernel must be a positive-definite function that maps two inputs, ð‘¥ and ð‘¥Ê¹, to a scalar, such that ð‘²ð±ð± is a valid covariance matrix.

For example, consider the single-dimensional inputs {ð‘¥â‚™} with a constant mean function at 0 and the following kernel:

```
ð‘˜(ð‘¥, ð‘¥Ê¹) = â„ŽÂ²(1 + ( (ð‘¥ ï¼ ð‘¥Ê¹)Â² / 2Î±ð‘™Â² )) ^ (ï¼Î±)
```

Where â„Ž, Î±, ð‘™ are positive hyperparameters âˆŠ â„. This is known as the **rational quadratic covariance** function (RQ). Note that it only depends on the inputs via their difference (ð‘¥ ï¼ ð‘¥Ê¹), meaning the shape of the function is constant throughout the input space. Further, as ð‘¥ and ð‘¥Ê¹ get closer together, the covariance is larger, resulting in continuity.

Some examples of the rational quadratic kernel function across various kernel parameters are shown below.

![Rational quadratic kernel across various kernel parameters.](http://keyonvafa.com/assets/images/gp_predictit_blog/gp_samples.png)

## Acknowledgements

> Please remember, this repo is intended as a personal revision guide and not an original tutorial, and as such some content is very similar to other sources which I found helpful in understanding this topic. ðŸ˜‡

My introduction to GPs was borrowed heavily from [keyonvafa.com](http://keyonvafa.com/gp-tutorial/), and I thought it would be helpful to repeat many of the same examples here.
